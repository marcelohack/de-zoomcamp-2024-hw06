{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8518f58-6ca2-431b-ba0b-532a74d2825b",
   "metadata": {},
   "source": [
    "# Data Engineering Zoomcamp 2024 - Homework 6\n",
    "## Question 6. Parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34de3ee6-d9f7-4236-acce-68d528bd65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614aecc8-b925-46ba-8f4b-cf127ff1b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "kafka_clients_jar = 'org.apache.kafka:kafka-clients:3.4.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5340ba2a-e4ab-47c9-87f9-e25539f91d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/mhack/.pyenv/versions/3.10.3/envs/pyspark-kafka/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/mhack/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/mhack/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e71eb352-1dda-4f23-92c2-b7ba74d9140a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-4 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in local-m2-cache\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in local-m2-cache\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in local-m2-cache\n",
      ":: resolution report :: resolve 212ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-4 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from local-m2-cache in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from local-m2-cache in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from local-m2-cache in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from local-m2-cache in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e71eb352-1dda-4f23-92c2-b7ba74d9140a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/4ms)\n",
      "24/03/14 15:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test_hw06_question6\") \\\n",
    "    .config(\"spark.jars.packages\", f\"{kafka_jar_package},{kafka_clients_jar}\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c450ae6-6c5b-4934-a170-25f00c8d86ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a9ea98-7239-4b98-885d-b41f3fbb3881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_stream.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63eab0fe-b767-45c8-af7d-fd9092128c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e568314e-41ed-401b-b356-cfcca7c77a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.IntegerType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24773777-45e6-4400-ae30-a3e1d100781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_schema(df_stream, schema):\n",
    "#     assert df_stream.isStreaming is True, 'DataFrame is not streaming'\n",
    "\n",
    "#     return df_stream \\\n",
    "#     .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "#     .select(\"data.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "726f6c47-dd27-46d7-a771-74eb6a01f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b944e992-c513-41f7-83e1-0081fe80caab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4260718-782f-4d80-8977-d3e3a72fbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_df.show()\n",
    "# Queries with streaming sources must be executed with writeStream.start();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a6240b-d657-4b72-814d-fa0caea5191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 15:24:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/g0/707fw8gn4y1dd16yfrl8xpkm0000gn/T/temporary-d1f97f69-8009-4413-9d0b-5b12673c954b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/14 15:24:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/14 15:24:13 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "|lpep_pickup_datetime|lpep_dropoff_datetime|PULocationID|DOLocationID|passenger_count|trip_distance|tip_amount|\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "| 2019-10-01 00:26:02|  2019-10-01 00:39:58|         112|         196|              1|         5.88|       0.0|\n",
      "| 2019-10-01 00:18:11|  2019-10-01 00:22:38|          43|         263|              1|          0.8|       0.0|\n",
      "| 2019-10-01 00:09:31|  2019-10-01 00:24:47|         255|         228|              2|          7.5|       0.0|\n",
      "| 2019-10-01 00:37:40|  2019-10-01 00:41:49|         181|         181|              1|          0.9|       0.0|\n",
      "| 2019-10-01 00:08:13|  2019-10-01 00:17:56|          97|         188|              1|         2.52|      2.26|\n",
      "| 2019-10-01 00:35:01|  2019-10-01 00:43:40|          65|          49|              1|         1.47|      1.86|\n",
      "| 2019-10-01 00:28:09|  2019-10-01 00:30:49|           7|         179|              1|          0.6|       1.0|\n",
      "| 2019-10-01 00:28:26|  2019-10-01 00:32:01|          41|          74|              1|         0.56|       0.0|\n",
      "| 2019-10-01 00:14:01|  2019-10-01 00:26:16|         255|          49|              1|         2.42|       0.0|\n",
      "| 2019-10-01 00:03:03|  2019-10-01 00:17:13|         130|         131|              1|          3.4|      2.85|\n",
      "| 2019-10-01 00:07:10|  2019-10-01 00:23:38|          24|          74|              3|         3.18|       0.0|\n",
      "| 2019-10-01 00:25:48|  2019-10-01 00:49:52|         255|         188|              1|          4.7|       1.0|\n",
      "| 2019-10-01 00:03:12|  2019-10-01 00:14:43|         129|         160|              1|          3.1|       0.0|\n",
      "| 2019-10-01 00:44:56|  2019-10-01 00:51:06|          18|         169|              1|         1.19|      0.25|\n",
      "| 2019-10-01 00:55:14|  2019-10-01 01:00:49|         223|           7|              1|         1.09|      1.46|\n",
      "| 2019-10-01 00:06:06|  2019-10-01 00:11:05|          75|         262|              1|         1.24|      2.01|\n",
      "| 2019-10-01 00:00:19|  2019-10-01 00:14:32|          97|         228|              1|         3.03|      3.58|\n",
      "| 2019-10-01 00:09:31|  2019-10-01 00:20:41|          41|          74|              1|         2.03|      2.16|\n",
      "| 2019-10-01 00:30:36|  2019-10-01 00:34:30|          41|          42|              1|         0.73|      1.26|\n",
      "| 2019-10-01 00:58:32|  2019-10-01 01:05:08|          41|         116|              1|         1.48|       0.0|\n",
      "+--------------------+---------------------+------------+------------+---------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .start()\n",
    "query.awaitTermination()\n",
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210c6d5-dc62-4b36-92e3-cd13106c5864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
